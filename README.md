# АНАЛИЗ ДАННЫХ В РАЗРАБОТКЕ ИГР [in GameDev]
Отчет по лабораторной работе #5 выполнил(а):
- Власова Елена Васильевна
- РИ230931
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Найдите внутри C# скрипта “коэффициент корреляции ” и сделать выводы о том, как он влияет на обучение модели.
- Задание 2.
- Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.
- Задание 3.
- Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения? 

- Выводы.
- ✨Magic ✨
# Машинное обучение в Unity. ML-Agent
# Цель работы : 
Познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.
## Постановка задачи.
В данной лабораторной работе мы создадим ML-агент и будем тренировать нейросеть, задача которой будет заключаться в управлении шаром. Задача шара заключается в том, чтобы оставаясь на плоскости находить кубик, смещающийся в заданном случайном диапазоне координат.

## 1 Поиск агентом объекта на сцене

Реализация системы машинного обучения в связке Python - Google-Sheetes-Unity:
- Создание нового пустого 3D проекта на Unity под названием ML-Agent_GameDiva
- 
![Снимок экрана 2024-12-15 194740](https://github.com/user-attachments/assets/024b1d18-0e23-4d9d-bd82-45437a169842)

- Скачала папку из Workshop#5: ml-agents-release_19.
- В проекте на Unity я с помощью команд Window - Package Manager - раскрыв + - Add Package From Disk добавила два .json файла:
- ml-agents-release_19 / com,unity.ml-agents / package.json
- ml-agents-release_19 / com,unity.ml-agents.extensions / package.json

![Снимок экрана 2024-12-16 130146](https://github.com/user-attachments/assets/6fd1e80a-e831-48c6-a3b8-4d7689aede7c)

![Снимок экрана 2024-12-16 130850](https://github.com/user-attachments/assets/d9c9b684-d260-4132-8c96-45ffca51a24a)

Во вкладке Component появился ML-agent, значит всё сделано правильно.

![Снимок экрана 2024-12-16 131900](https://github.com/user-attachments/assets/93acde91-d5fc-4f7f-892e-fad8b31bdee0)

Запуск Anaconda Promt для дальнейшей работы:
- создание ML-агента под именем GameDiva
![Снимок экрана 2024-12-16 141936](https://github.com/user-attachments/assets/9e2d3a93-8fb0-4f21-bf58-4a261ae89d06)

- Подгрузка нужных файлов
- Активация ML-агента GameDiva
- Переход к работе сразу с папкой проекта Unity C:\Users\712\ML-Agent_GameDIva
![Снимок экрана 2024-12-16 141720](https://github.com/user-attachments/assets/59e7b40b-9f4d-474e-be71-70d371840f00)
- Установка библиотек

![Снимок экрана 2024-12-16 151254](https://github.com/user-attachments/assets/8c399b2d-ee6d-407c-9956-de650557f9c6)

![Снимок экрана 2024-12-16 151318](https://github.com/user-attachments/assets/a8e294b8-ebbc-4ea7-8420-7d9bd4ae2c1a)

![Снимок экрана 2024-12-16 154806](https://github.com/user-attachments/assets/5cd9d9d2-f4ce-4063-8983-600dcd4262b5)

- Создание на сцене плоскости, куба и сферы
- Создание C# скрипта и подключение его к сфере

![Снимок экрана 2024-12-16 163610](https://github.com/user-attachments/assets/8cc855c9-03be-4bad-940f-510fdeda6954)

- Доабвление кода к сфере из материалов лабораторных работ

![Снимок экрана 2024-12-16 164001](https://github.com/user-attachments/assets/c3e3ec13-c6e6-4be0-88ad-10510fb42b8f)

![Снимок экрана 2024-12-16 164007](https://github.com/user-attachments/assets/6d8985e6-2a3d-45a6-b0f9-370c67bc85f6)

- Добавление к сфере Rigitbody, Decision Requester, Behavior Parameters и их настройка

![Снимок экрана 2024-12-16 170045](https://github.com/user-attachments/assets/836e9bdb-f9e0-4f24-ad05-968cd9240c48)

![Снимок экрана 2024-12-16 170105](https://github.com/user-attachments/assets/2b01bb66-473f-46d8-bdee-891d46633531)

- Добавление в корень проекта файла конфигурации нейронной сети

![Снимок экрана 2024-12-16 173412](https://github.com/user-attachments/assets/10b2b87b-a6ac-4913-978c-7b4f57a7b92a)

- Запуск работы ML-агента

![Снимок экрана 2024-12-16 185951](https://github.com/user-attachments/assets/6e81befc-386e-47b7-bc65-f1e57cafa7eb)

![Снимок экрана 2024-12-16 185959](https://github.com/user-attachments/assets/04ab10a0-db12-436b-ae69-2a6e8e18ac4b)

- Перенос кубика, сферы и поверхности в один пустой объект TargetArea, создание из него префаба
- Создание 3-х копий ML-агента, проверка их работоспособности
- Небольшой вывод: как и в случае с одним ML-агентом, в Anaconda Promt очень долго ждать даже одного результата обучения
![Снимок экрана 2024-12-17 201317](https://github.com/user-attachments/assets/d3618c08-3198-4aee-9d0c-af5eada2486d)

- Создание 9-ти копий ML-агента, проверка их работоспособности
- Небольшой вывод: результат обучения считается гораздо быстрее
![Снимок экрана 2024-12-17 201641](https://github.com/user-attachments/assets/1ef075ae-4a64-44a6-8a4f-7ea9b6fe9a2d)

![Снимок экрана 2024-12-17 201814](https://github.com/user-attachments/assets/6f97c744-a213-425e-877c-0cb3013459bf)

- Создание 27-ми копий ML-агента, проверка их работоспособности
- Небольшой вывод: результат обучения считается быстрее, если сравнить с быстротой обучения 9-ти моделек, разрыв в обучении начинает только возрастать.
![Снимок экрана 2024-12-17 203406](https://github.com/user-attachments/assets/c7ab5ae0-e005-4386-b96c-4e59cd502250)

![Снимок экрана 2024-12-17 203527](https://github.com/user-attachments/assets/c6c1d9f6-b006-4e33-a6dc-24fe95f69529)

- Я сохраняла все результаты кнопками Ctrl + C, они же отобразились в папке results, из неё нужно скопировать файл RollerBall.onnx и вставить в папку Assets.

![Снимок экрана 2024-12-17 204842](https://github.com/user-attachments/assets/3c156549-a810-4c11-9894-063c0f8b690d)

- После чего при возвращении в Unity можно увидеть RollerBall, как один из элементов, которыми можно управлять, а конкретнее поместить у RollerAgent в инспекторе Behavior Parameters в поле Model, при этом задать свойство Behavior Type - Inference Only
![Снимок экрана 2024-12-17 210236](https://github.com/user-attachments/assets/afdee806-3a15-47b8-9df4-e778586e917c)

- После этого снова проверка ML-агента, он работает и уже визуально можно сказать, что шарик не выкатывается за края плоскости, а быстрее и следует за кубиком при его перемещении в другую сторону.
![Снимок экрана 2024-12-17 210323](https://github.com/user-attachments/assets/d9a726fb-a641-4268-bf61-ecff5c5ad5bd)

![Снимок экрана 2024-12-17 210652](https://github.com/user-attachments/assets/3d5f715f-c1aa-4757-a937-94015bd92458)

Глобальный вывод по проделанной работе: если в инспекторе в качестве модели прикрепить зараннее обученную на 27 ML-агентах нейронную сеть, то одна такая моделька ML-агента по скорости обучения равнозначна 27-ми ML-агентам, которые обучаются впервые, также стоит отметить, что если сравнить 9 моделек и 27, то скорость обучения будет выигрывать у 27-ми при увеличении времени обучения 9 моделек начнут существенно отставать. Также стоит отметить, что не стоит считать эффективным обучения одной или даже 3-х моделек (к которым не была прикреплена обученная нейронная сеть), потому что оно будет проходить очень медленно, и с большим трудом удастся дождаться хотя бы одного успешного обучения

## 2 Симулятор добычи ресурсов
- Скачала проект Unity ML-Agent_EconomicModel и открыла его на Unity, добавив в него два JSON файла ML Agent и ML Agent Extensions (без них были ошибки)

![Снимок экрана 2024-12-17 231513](https://github.com/user-attachments/assets/c6ce7e2e-b858-4b7e-9fbd-8a3c695e7503)

- Знакомство с его работой. Так как файл Economic.yaml уже был в папке проекта, то пункт уже считается выполненным.
- Запуск Anaconda Promt от имени администратора.
- Создание нового виртуального пространства под именем GameDiva2 и подгрузка библиотек

![Снимок экрана 2024-12-18 161758](https://github.com/user-attachments/assets/2113cb7e-0a9f-40c9-bdf8-83828e3f60db)

- Активация созданного пространства и установка двух пакетов в нём

![Снимок экрана 2024-12-18 162521](https://github.com/user-attachments/assets/7af4fdb1-9417-4417-a7f1-ba1f45bba119)

![Снимок экрана 2024-12-18 162610](https://github.com/user-attachments/assets/cc816267-7b81-4f7d-b1d3-98207b488572)

- Переход в папку, в которой лежит проект с файлом Economic.yaml

![Снимок экрана 2024-12-18 163357](https://github.com/user-attachments/assets/e16b724f-f9c6-4a72-99f4-d6d08dc2a199)

- Запуск обучения ML-агента mlagents-learn Economic.yaml --run-id=Economic –force
![Снимок экрана 2024-12-18 163925](https://github.com/user-attachments/assets/f7cf422a-d300-443c-9c9d-dca114b8dca9)

![Снимок экрана 2024-12-18 164027](https://github.com/user-attachments/assets/9c4e75ec-58b2-48ec-b783-913255b2eb42)

- Так как я получила результаты обучения в командной строке, то всё сделано правильно. Были сообщения о том, что обучение не произведено, и о том, что обучение было успешным, но большинство сообщений было о том, что обучение не было произведено.

![Снимок экрана 2024-12-18 164205](https://github.com/user-attachments/assets/26dbb5d9-ee2c-4624-9793-6f4a584faf49)

- Завершала я обучение сочетанием клавиш Ctrl + C, чтобы результат был сохранен. Результат был сохранён в папке results в виде обученной нейронной сети

![Снимок экрана 2024-12-18 165427](https://github.com/user-attachments/assets/e2816395-bbb1-4ac4-a528-f9adeb873b68)

- Установка TensorBoard в командной строке

![Снимок экрана 2024-12-18 165950](https://github.com/user-attachments/assets/97019d09-487e-40d0-869b-1cf1651ccfc7)

- Запуск TensorBoard

![Снимок экрана 2024-12-18 170112](https://github.com/user-attachments/assets/e76d0e38-b20e-4c4e-847b-e63bd4824880)

Запуск TensorBoard по ссылке http://localhost:6006

![Снимок экрана 2024-12-18 170455](https://github.com/user-attachments/assets/1857fa92-dd95-4ca1-ad78-385cc397a07f)

## Задание 1
### Найдите внутри C# скрипта “коэффициент корреляции ” и сделать выводы о том, как он влияет на обучение модели.

Ход работы:
- Думаю имеет смысл перейти к C# скрипту RollerAgent, так как он есть в обоих проектах, в отличие от скрипта Move, который описывает передвижение шарика. Исходя из знаний прошлых лекций про перцептрон и его обучение, значение 1.42f напоминает пороговое значение, ознаающее выдавать или нет награду агенту.
- Например, в первом проекте, шарику надо коснуться до цели - зеленого кубика, и тогда награда будет получена. А этот коэффициент как раз таки определяет, насколько близко агенту надо приблизиться цели, чтобы награда была засчитана. То есть, если бы это значение было намного больше, то шарику достаточно лишь на пару значений сдвинуться в сторону кубика, чтобы награда была бы уже выдана. Но с другой стороны из-за такого значения очень сильно пострадает качество обучения. Или наоборот, чтобы повысить качество обучения, надо уменьшить этот коэффициент, тогда награда станет совсем редкостью.
- Вывод: я считаю, что коэффициентом корреляции в скрипте RollerAgent является значение 1.42f, именно он является пороговым значением для выдачи награды агенту и именно он задаёт темп обучения и его скорость.

![Снимок экрана 2024-12-18 224318](https://github.com/user-attachments/assets/fb2f8d0b-000d-40c3-a1ac-e67d53eadc20)

## Задание 2
### Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.

Ход работы:

## Задание 3
### Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения? 

Ход работы:

## Выводы

Абзац умных слов о том, что было сделано и что было узнано.

| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
